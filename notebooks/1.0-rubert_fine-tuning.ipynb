{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4333d04a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effaf3fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_random_seed(seed_val: int = 66) -> None:\n",
    "    \"\"\"\n",
    "    Fixes random seed\n",
    "    @param seed_val: value for random seed\n",
    "    \"\"\"\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe30a9f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('best_df.csv')\n",
    "df.target = df.target - np.repeat(1, df.shape[0])\n",
    "text_column = df.text\n",
    "\n",
    "\n",
    "def get_input_ids_max_len(df_text_column: pd.Series, tokenizer: any) -> int:\n",
    "    \"\"\"\n",
    "    @param df_text_column: column to be processed by the tokenizer\n",
    "    @param tokenizer: tokenizer object\n",
    "    @return: max length of input ids from the tokenizer\n",
    "    \"\"\"\n",
    "    input_ids = df_text_column.apply(\n",
    "        lambda x: tokenizer.encode(x, add_special_tokens=True)\n",
    "    )\n",
    "    return max(input_ids.apply(len))\n",
    "\n",
    "\n",
    "def get_tokenizer(model_checkpoint: str) -> any:\n",
    "    \"\"\"\n",
    "    @param model_checkpoint: name of the model checkpoint\n",
    "    @return: tokenizer object\n",
    "    \"\"\"\n",
    "    return AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'\n",
    "tokenizer = get_tokenizer(model_checkpoint)\n",
    "max_len = get_input_ids_max_len(text_column, tokenizer)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0462aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 55669, 7471, 43148, 12945, 10167, 11466, 852, 10770, 37823, 1388, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text_field(df_text_column: pd.Series, tokenizer: any, max_len: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    @param df_text_column: column to be processed by the tokenizer\n",
    "    @param tokenizer: tokenizer object\n",
    "    @param max_len: max length of input ids from the tokenizer\n",
    "    @return: pandas column with dictionary of input_ids, token_type_ids and attention_mask\n",
    "    for each text in the dataframe\n",
    "    \"\"\"\n",
    "    return df_text_column.apply(\n",
    "        lambda x: tokenizer.encode_plus(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,  # padding='longest' does not work correctly in this version\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_text = tokenize_text_field(text_column, tokenizer, max_len)\n",
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de6cc22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101., 55669.,  7471., 43148., 12945., 10167., 11466.,   852., 10770.,\n",
       "        37823.,  1388.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_offset_and_specify_torch_datatype(\n",
    "    tokenized_text: pd.Series, offset: str\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    @param tokenized_text: pandas column with dictionary of input_ids, token_type_ids and attention_mask\n",
    "    for each text in the dataframe\n",
    "    @param offset: offset from the tokenized text dictionary\n",
    "    @return: torch tensor of specified data\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenized_text.apply(lambda x: x[offset]), dtype=torch.float64)\n",
    "\n",
    "\n",
    "input_ids = get_offset_and_specify_torch_datatype(\n",
    "    tokenized_text, \"input_ids\"\n",
    ")\n",
    "token_type_ids = get_offset_and_specify_torch_datatype(\n",
    "    tokenized_text, \"token_type_ids\"\n",
    ")\n",
    "attention_mask = get_offset_and_specify_torch_datatype(\n",
    "    tokenized_text, \"attention_mask\"\n",
    ")\n",
    "labels = torch.tensor(df.target, dtype=torch.float64)\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26328f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101., 71911., 12945., 18551., 39701.,   852., 13236.,  7494., 12986.,\n",
       "           862.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor(11., dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset(\n",
    "    input_ids: torch.tensor,\n",
    "    token_type_ids: torch.tensor,\n",
    "    attention_mask: torch.tensor,\n",
    "    labels: torch.tensor,\n",
    ") -> TensorDataset:\n",
    "    \"\"\"\n",
    "    @param input_ids: input_ids from the tokenizer\n",
    "    @param token_type_ids: token_type_ids from the tokenizer\n",
    "    @param attention_mask: attention_mask from the tokenizer\n",
    "    @param labels: object labels\n",
    "    @return: full TensorDataset object\n",
    "    \"\"\"\n",
    "    return TensorDataset(input_ids, token_type_ids, attention_mask, labels)\n",
    "\n",
    "\n",
    "def get_train_val_stratified_dataset(\n",
    "    full_dataset: TensorDataset,\n",
    "    labels: torch.tensor,\n",
    "    test_size: float,\n",
    ") -> tuple[torch.utils.data.Subset, torch.utils.data.Subset]:\n",
    "    \"\"\"\n",
    "    @param full_dataset: full TensorDataset object\n",
    "    @param labels: object labels\n",
    "    @param test_size: test sample size\n",
    "    @return: stratified train and test datasets\n",
    "    \"\"\"\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(labels))), test_size=test_size, stratify=labels\n",
    "    )\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def get_data_loader(dataset, batch_size: int) -> DataLoader:\n",
    "    \"\"\"\n",
    "    @param dataset: train or test dataset\n",
    "    @param batch_size: size of one batch in dataloader object\n",
    "    @return: dataloader object\n",
    "    \"\"\"\n",
    "    return DataLoader(\n",
    "        dataset, sampler=SequentialSampler(dataset), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "full_dataset = get_dataset(input_ids, token_type_ids, attention_mask, labels)\n",
    "train_dataset, val_dataset = get_train_val_stratified_dataset(\n",
    "    full_dataset, labels, test_size\n",
    ")\n",
    "train_dataloader = get_data_loader(train_dataset, batch_size)\n",
    "validation_dataloader = get_data_loader(val_dataset, batch_size)\n",
    "# full_dataloader = get_data_loader(full_dataset, batch_size)\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013bc490",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_and_labels_flatted(\n",
    "    logits: np.ndarray, true_ids: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    @param logits: model predictions\n",
    "    @param true_ids: true labels\n",
    "    @return: flatted model predictions and true labels\n",
    "    \"\"\"\n",
    "    preds_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = true_ids.flatten()\n",
    "\n",
    "    return preds_flat, labels_flat\n",
    "\n",
    "\n",
    "def get_all_metrics(preds: np.ndarray, true_label_ids: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dictionary of all necessary metrics\n",
    "    @param preds: flatted model predictions\n",
    "    @param true_label_ids: flatted true labels\n",
    "    @return: metric values\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score(preds, true_label_ids),\n",
    "        \"recall_score\": recall_score(preds, true_label_ids, average=\"micro\"),\n",
    "        \"precision_score\": precision_score(preds, true_label_ids, average=\"micro\"),\n",
    "        \"f1_score\": f1_score(preds, true_label_ids, average=\"micro\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbaa48e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-base-cased-nli-threeway and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def get_device() -> str:\n",
    "    \"\"\"\n",
    "    @return: device name\n",
    "    \"\"\"\n",
    "    return \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_model(model_checkpoint: str, num_labels: int) -> any:\n",
    "    \"\"\"\n",
    "    @param model_checkpoint: name of the model checkpoint\n",
    "    @param num_labels: number of labels in the dataframe\n",
    "    @return: model object\n",
    "    \"\"\"\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=num_labels, ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "model = get_model(model_checkpoint, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "584caded",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def val_model(\n",
    "    model: any,\n",
    "    validation_dataloader: DataLoader,\n",
    "    device: str,\n",
    ") -> None:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_metrics = {\n",
    "        \"accuracy_score\": 0,\n",
    "        \"recall_score\": 0,\n",
    "        \"precision_score\": 0,\n",
    "        \"f1_score\": 0,\n",
    "    }\n",
    "\n",
    "    eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = torch.tensor(batch[0]).to(device).long()\n",
    "        b_token_type_ids = torch.tensor(batch[1]).to(device).long()\n",
    "        b_attention_mask = torch.tensor(batch[2]).to(device).long()\n",
    "        b_labels = torch.tensor(batch[3]).to(device).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=b_token_type_ids,\n",
    "                attention_mask=b_attention_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        flat_preds, flat_label_ids = get_preds_and_labels_flatted(logits, label_ids)\n",
    "        metric_results = get_all_metrics(flat_preds, flat_label_ids)\n",
    "\n",
    "        for metric in metric_results.keys():\n",
    "            eval_metrics[metric] += metric_results[metric]\n",
    "\n",
    "    for metric in eval_metrics.keys():\n",
    "        metric_value = eval_metrics[metric] / len(validation_dataloader)\n",
    "        print(f'{metric}: {metric_value:.4f}')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27deccc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50  of  807.    Spent: 19.881248474121094. Current_loss 0.036512114107608795\n",
      "Batch 100  of  807.    Spent: 39.22720432281494. Current_loss 0.02678094431757927\n",
      "Batch 150  of  807.    Spent: 58.609485387802124. Current_loss 0.0207226425409317\n",
      "Batch 200  of  807.    Spent: 78.03159832954407. Current_loss 0.03630485758185387\n",
      "Batch 250  of  807.    Spent: 97.50578308105469. Current_loss 0.017175067216157913\n",
      "Batch 300  of  807.    Spent: 116.91290426254272. Current_loss 0.015692181885242462\n",
      "Batch 350  of  807.    Spent: 136.40206289291382. Current_loss 0.03139745816588402\n",
      "Batch 400  of  807.    Spent: 155.90159344673157. Current_loss 0.024047108367085457\n",
      "Batch 450  of  807.    Spent: 175.41085481643677. Current_loss 0.030946440994739532\n",
      "Batch 500  of  807.    Spent: 194.85094666481018. Current_loss 0.020916113629937172\n",
      "Batch 550  of  807.    Spent: 214.31190395355225. Current_loss 0.03688619285821915\n",
      "Batch 600  of  807.    Spent: 233.79784870147705. Current_loss 0.028216492384672165\n",
      "Batch 650  of  807.    Spent: 253.25468635559082. Current_loss 0.027518903836607933\n",
      "Batch 700  of  807.    Spent: 272.72398471832275. Current_loss 0.02893471159040928\n",
      "Batch 750  of  807.    Spent: 292.1839954853058. Current_loss 0.015421560034155846\n",
      "Batch 800  of  807.    Spent: 311.6208236217499. Current_loss 0.029071524739265442\n",
      "Average training loss: 0.00\n",
      "Training epcoh took: 313.9364755153656\n",
      "accuracy_score: 0.7737\n",
      "recall_score: 0.7737\n",
      "precision_score: 0.7737\n",
      "f1_score: 0.7737\n",
      "Batch 50  of  807.    Spent: 19.803849458694458. Current_loss 0.020721737295389175\n",
      "Batch 100  of  807.    Spent: 39.28318548202515. Current_loss 0.014797860756516457\n",
      "Batch 150  of  807.    Spent: 58.790879249572754. Current_loss 0.018645716831088066\n",
      "Batch 200  of  807.    Spent: 78.29621863365173. Current_loss 0.02984379418194294\n",
      "Batch 250  of  807.    Spent: 97.75137257575989. Current_loss 0.016180960461497307\n",
      "Batch 300  of  807.    Spent: 117.17017889022827. Current_loss 0.0111202048137784\n",
      "Batch 350  of  807.    Spent: 136.56022214889526. Current_loss 0.023806419223546982\n",
      "Batch 400  of  807.    Spent: 156.0021526813507. Current_loss 0.01828353852033615\n",
      "Batch 450  of  807.    Spent: 175.4196479320526. Current_loss 0.025186453014612198\n",
      "Batch 500  of  807.    Spent: 194.86738681793213. Current_loss 0.022504141554236412\n",
      "Batch 550  of  807.    Spent: 214.3304443359375. Current_loss 0.03044254332780838\n",
      "Batch 600  of  807.    Spent: 233.8719198703766. Current_loss 0.019979441538453102\n",
      "Batch 650  of  807.    Spent: 253.29559350013733. Current_loss 0.01966116949915886\n",
      "Batch 700  of  807.    Spent: 272.7287173271179. Current_loss 0.023645153269171715\n",
      "Batch 750  of  807.    Spent: 292.2674751281738. Current_loss 0.014459066092967987\n",
      "Batch 800  of  807.    Spent: 311.7861638069153. Current_loss 0.02464212477207184\n",
      "Average training loss: 0.00\n",
      "Training epcoh took: 314.12081694602966\n",
      "accuracy_score: 0.7784\n",
      "recall_score: 0.7784\n",
      "precision_score: 0.7784\n",
      "f1_score: 0.7784\n",
      "Batch 50  of  807.    Spent: 19.93797731399536. Current_loss 0.017201319336891174\n",
      "Batch 100  of  807.    Spent: 39.4522647857666. Current_loss 0.011856337077915668\n",
      "Batch 150  of  807.    Spent: 59.010875940322876. Current_loss 0.011735181324183941\n",
      "Batch 200  of  807.    Spent: 78.50548720359802. Current_loss 0.027429679408669472\n",
      "Batch 250  of  807.    Spent: 98.0644953250885. Current_loss 0.011911377310752869\n",
      "Batch 300  of  807.    Spent: 117.5800609588623. Current_loss 0.006507223006337881\n",
      "Batch 350  of  807.    Spent: 137.06875681877136. Current_loss 0.01804949715733528\n",
      "Batch 400  of  807.    Spent: 156.55833506584167. Current_loss 0.012862471863627434\n",
      "Batch 450  of  807.    Spent: 176.07556867599487. Current_loss 0.0183779988437891\n",
      "Batch 500  of  807.    Spent: 195.5752592086792. Current_loss 0.01675075851380825\n",
      "Batch 550  of  807.    Spent: 215.11036562919617. Current_loss 0.025075122714042664\n",
      "Batch 600  of  807.    Spent: 234.62135100364685. Current_loss 0.01812378317117691\n",
      "Batch 650  of  807.    Spent: 254.11755990982056. Current_loss 0.016203833743929863\n"
     ]
    }
   ],
   "source": [
    "def train_model(\n",
    "    model: any,\n",
    "    device: str,\n",
    "    num_epochs: int,\n",
    "    train_dataloader: DataLoader,\n",
    "    validation_dataloader: DataLoader,\n",
    ") -> any:\n",
    "    \"\"\"\n",
    "    @param model: model object\n",
    "    @param device: device name\n",
    "    @param num_epochs: number of model learning epochs\n",
    "    @param train_dataloader: processed dataloader object for model training\n",
    "    @return: trained model object\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = train_dataloader.batch_size\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        t0 = time.time()\n",
    "        train_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            b_input_ids = torch.tensor(batch[0]).to(device).long()\n",
    "            b_token_type_ids = torch.tensor(batch[1]).to(device).long()\n",
    "            b_attention_mask = torch.tensor(batch[2]).to(device).long()\n",
    "            b_labels = torch.tensor(batch[3]).to(device).long()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=b_input_ids,\n",
    "                token_type_ids=b_token_type_ids,\n",
    "                attention_mask=b_attention_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            train_loss = loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if step % 50 == 0 and not step == 0:\n",
    "                spent = time.time() - t0\n",
    "\n",
    "                current_loss = train_loss / batch_size\n",
    "\n",
    "                print(\n",
    "                    \"Batch {:}  of  {:}.    Spent: {:}. Current_loss {:}\".format(\n",
    "                        step, len(train_dataloader), spent, current_loss\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        training_time = time.time() - t0\n",
    "\n",
    "        print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"Training epcoh took: {:}\".format(training_time))\n",
    "        val_model(model, validation_dataloader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "train_model(model, device, 3, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d36c34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 66 - 0.7893"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}